<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-05-16T23:24:25-07:00</updated><id>http://localhost:4000/</id><title type="html">Random Thoughts</title><subtitle>Personal Blog about Technology and Business
</subtitle><entry><title type="html">LLVM vs Dalvik (Android) note II</title><link href="http://localhost:4000/compiler/2013/10/30/LLVM2.html" rel="alternate" type="text/html" title="LLVM vs Dalvik (Android) note II" /><published>2013-10-30T00:00:00-07:00</published><updated>2013-10-30T00:00:00-07:00</updated><id>http://localhost:4000/compiler/2013/10/30/LLVM2</id><content type="html" xml:base="http://localhost:4000/compiler/2013/10/30/LLVM2.html">&lt;p&gt;Dalvik is the process virtual machine in Google&amp;#39;s &lt;em&gt;Android&lt;/em&gt; operating system. As we all known, Andorid right now is the most popular mobile platform. &lt;/p&gt;

&lt;h5&gt;Differences&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Code generation&lt;/em&gt; , Bitcode can be translated to native code by LLVM and during the process it also can call Native lib and be excuted by cpu. But for Dalvik ByteCode, it must be excuted under Dalvik VM. Even with Dalvik JIT compiler, there is only small part of Dalvik ByteCode TraceRun module can be translated to the native assembly. It is not like the native code generated by LLVM can be excuted 100% in native environment.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Reuseability&lt;/em&gt; , LLVM can optimize BitCode into native code entirely. Now, Dalvik ByteCode only support runtime ByteCode JIT compiler. Once Dalvik program exits, all JIT compiled results are gone. It needs the Dalvik program be reloaded and then decide which part to be translated into native code by every ByteCode Trace-Run&amp;#39;s counter.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Space&lt;/em&gt; , Usually, Dalvik application APK nees 2 storage space, one for DEX , one for ODEX that stays in dalvik cache. But LLVM application don&amp;#39;t need to do that.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Computing Resource&lt;/em&gt; , The result from LLVM compiler can be excuted in a native way but Dalvik JIT needs Trace-Run Counter to decide recompiling. So from cpu runtim overhead, LLVM is more efficient.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Security&lt;/em&gt;, LLVM suport inline assembly which is not allowed in Java world. This makes LLVM efficient but also some security problem around this.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;Dalvik Compiling&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/LLVM2.png&quot; alt=&quot;Dalvik&quot; title=&quot;Dalvik&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;LLVM Compiling&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/LLVM3.png&quot; alt=&quot;LLVM&quot; title=&quot;LLVM&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="LLVM" /><summary type="html">Dalvik is the process virtual machine in Google&amp;#39;s Android operating system. As we all known, Andorid right now is the most popular mobile platform.</summary></entry><entry><title type="html">LLVM note I</title><link href="http://localhost:4000/compiler/2013/10/29/LLVM.html" rel="alternate" type="text/html" title="LLVM note I" /><published>2013-10-29T00:00:00-07:00</published><updated>2013-10-29T00:00:00-07:00</updated><id>http://localhost:4000/compiler/2013/10/29/LLVM</id><content type="html" xml:base="http://localhost:4000/compiler/2013/10/29/LLVM.html">&lt;h4&gt;Cross Platform&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/LLVM1.png&quot; alt=&quot;LLVM&quot; title=&quot;LLVM&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;LLVM uses clang as front end to translate c/c++/java/fortran to bitcode representation and then LLVM compiler will convert this to different platform&amp;#39;s assembly.&lt;/p&gt;

&lt;p&gt;Clang is not only the front end of LLVM for c/c++/objective c  compiling, but also a &lt;em&gt;static analyzer&lt;/em&gt; to help the development of software. Well, in the industry, there are couple static analyzer like &lt;em&gt;Coverity&lt;/em&gt; , &lt;em&gt;Klocwork&lt;/em&gt;. They can analysis the source code at development stage statically. That can save a lot effort in debugging in the runtime.&lt;/p&gt;

&lt;h4&gt;SSA( static single assignment )&lt;/h4&gt;

&lt;p&gt;LLVM IR(&lt;em&gt;intermediary representation&lt;/em&gt;) is in the form of SSA. SSA should be the most influencial project because this is the base of the design of LLVM Assembly. SSA was originally be introduced in 1988 by Wegman, Zadeck , Alpern. Right now, it is in the production status with GCC 4.0 or Java JIT compiler. The basic idea of SSA is to limit the variable assignment counts to only 1. If a variable is changed 5 times in the computing, it will generate 5 copies of this variable.&lt;/p&gt;</content><author><name></name></author><category term="LLVM" /><summary type="html">Cross Platform LLVM uses clang as front end to translate c/c++/java/fortran to bitcode representation and then LLVM compiler will convert this to different platform&amp;#39;s assembly. Clang is not only the front end of LLVM for c/c++/objective c compiling, but also a static analyzer to help the development of software. Well, in the industry, there are couple static analyzer like Coverity , Klocwork. They can analysis the source code at development stage statically. That can save a lot effort in debugging in the runtime. SSA( static single assignment ) LLVM IR(intermediary representation) is in the form of SSA. SSA should be the most influencial project because this is the base of the design of LLVM Assembly. SSA was originally be introduced in 1988 by Wegman, Zadeck , Alpern. Right now, it is in the production status with GCC 4.0 or Java JIT compiler. The basic idea of SSA is to limit the variable assignment counts to only 1. If a variable is changed 5 times in the computing, it will generate 5 copies of this variable.</summary></entry><entry><title type="html">Implementing a compiler by LLVM and OCaml</title><link href="http://localhost:4000/system/2013/10/27/COMP.html" rel="alternate" type="text/html" title="Implementing a compiler by LLVM and OCaml" /><published>2013-10-27T00:00:00-07:00</published><updated>2013-10-27T00:00:00-07:00</updated><id>http://localhost:4000/system/2013/10/27/COMP</id><content type="html" xml:base="http://localhost:4000/system/2013/10/27/COMP.html">&lt;p&gt;I came accross the open source compiler project &amp;quot;LLVM&amp;quot; because sometimes I find GCC is not quite efficient for large project with many modules. I did some search and found out LLVM is a pretty cool open source compiler project and a lot of innovations are being developing on top that. I had experience with writing compiler in OCaml. So I&amp;#39;m gonna implement a simple language by using LLVM and OCaml for purpose of getting used to LLVM.  My ideal language project would be some language similar with Haskell or Go for supporting Concurrent Programming in natrue.  I will keep posting progress about this project. I think it will have a lot of fun. &lt;/p&gt;

&lt;p&gt;Hopefully, I can finish it in 1 month by using my entire spare time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jovechiang/llvmprac&quot;&gt;code link&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="compiler" /><summary type="html">I came accross the open source compiler project &amp;quot;LLVM&amp;quot; because sometimes I find GCC is not quite efficient for large project with many modules. I did some search and found out LLVM is a pretty cool open source compiler project and a lot of innovations are being developing on top that. I had experience with writing compiler in OCaml. So I&amp;#39;m gonna implement a simple language by using LLVM and OCaml for purpose of getting used to LLVM. My ideal language project would be some language similar with Haskell or Go for supporting Concurrent Programming in natrue. I will keep posting progress about this project. I think it will have a lot of fun.</summary></entry><entry><title type="html">Lock Free Data Structure I</title><link href="http://localhost:4000/algorithm/2013/10/15/LFREE.html" rel="alternate" type="text/html" title="Lock Free Data Structure I" /><published>2013-10-15T00:00:00-07:00</published><updated>2013-10-15T00:00:00-07:00</updated><id>http://localhost:4000/algorithm/2013/10/15/LFREE</id><content type="html" xml:base="http://localhost:4000/algorithm/2013/10/15/LFREE.html">&lt;h4&gt;Introduction&lt;/h4&gt;

&lt;p&gt;In my current work, there are many heavy work needs CP and IO operation. In order to acheive a high throughput, we adapt Producer and Consumer pattern. But to maintain the state is really harmful for system performance. So I did some research and try to avoid that. This post will introduce some lock free data structure and lock free queue can be used for Producer/Consumer pattern. And in my project, that actually improve QPS &lt;em&gt;10x&lt;/em&gt;!!! Awesome!!! &lt;/p&gt;

&lt;h4&gt;Compare&lt;em&gt;And&lt;/em&gt;Swap&lt;/h4&gt;

&lt;p&gt;Wiki:&lt;br&gt;
Compare&lt;em&gt;And&lt;/em&gt;Swap (CAS) is an atomic instruction used in multithreading to achieve synchronization.It compares the contents of a memory location to a given value and, only if they are the same, modifies the contents of that memory location to a given new value. This is done as a single atomic operation. The atomicity guarantees that the new value is calculated based on up-to-date information; if the value had been updated by another thread in the meantime, the write would fail. The result of the operation must indicate whether it performed the substitution; this can be done either with a simple Boolean response (this variant is often called compare-and-set), or by returning the value read from the memory location (not the value written to it).&lt;/p&gt;

&lt;p&gt;CAS is supported directly from CPU level, which means it ensure the atomicity. We can describe the behavior of CAS by following function:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;compare_and_swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oldval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ATOMIC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_reg_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_reg_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oldval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;END_ATOMIC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_reg_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;CAS is not only used for implementing synchronization primitives like semaphores and mutexes but also used for more sophisticated &lt;em&gt;lock-free and wait-free algorithms&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;By using CAS syntax in &lt;em&gt;C++ 11&lt;/em&gt; (compare&lt;em&gt;exchange&lt;/em&gt;weak), we can define a non blocking counter:  &lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;nonblocking_counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;atomic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// increment the count
&lt;/span&gt;    &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;increment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// get old value and calculate new value for counter
&lt;/span&gt;        &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// Atomically increment the counter.
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;boolean&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;success&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compare_exchange_weak&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// someone else changed the counter first -- start over.
&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// get the current count
&lt;/span&gt;    &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;ABA problem&lt;/h4&gt;

&lt;p&gt;Before we start implement this, first I want to introduce a famous problem &lt;em&gt;ABA&lt;/em&gt; in CAS :It&amp;#39;s possible that between the time the old value is read and the time CAS is attempted, some other processors or threads change the memory location two or more times such that it acquires a bit pattern which matches the old value. The problem arises if this new bit pattern, which looks exactly like the old value, has a different meaning: for instance, it could be a recycled address, or a wrapped version counter.&lt;/p&gt;

&lt;p&gt;A general solution to this is to use a double-length CAS (e.g on a 32 bit system, a 64 bit CAS). The second half is used to hold a counter. The compare part of the operation compares the previously read value of the pointer &lt;em&gt;and&lt;/em&gt; the counter, to the current pointer and counter. If they match, the swap occurs(the new value is written) but the new value has an incremented counter. This means that if &lt;em&gt;ABA&lt;/em&gt; has occurred, although the pointer value will be the same, the counter is exceedingly unlikedly to be the same.&lt;/p&gt;

&lt;h4&gt;Reference&lt;/h4&gt;

&lt;p&gt;Maurice Herlihy (1991)
Wikipedia&lt;/p&gt;</content><author><name></name></author><category term="Lock-Free" /><summary type="html">Introduction In my current work, there are many heavy work needs CP and IO operation. In order to acheive a high throughput, we adapt Producer and Consumer pattern. But to maintain the state is really harmful for system performance. So I did some research and try to avoid that. This post will introduce some lock free data structure and lock free queue can be used for Producer/Consumer pattern. And in my project, that actually improve QPS 10x!!! Awesome!!!</summary></entry><entry><title type="html">Caching Strategies</title><link href="http://localhost:4000/system/2013/10/09/CACHE.html" rel="alternate" type="text/html" title="Caching Strategies" /><published>2013-10-09T00:00:00-07:00</published><updated>2013-10-09T00:00:00-07:00</updated><id>http://localhost:4000/system/2013/10/09/CACHE</id><content type="html" xml:base="http://localhost:4000/system/2013/10/09/CACHE.html">&lt;h4&gt;Background&lt;/h4&gt;

&lt;p&gt;In current tech world, performance and scaling are 2 main factors when we design a system. Cache is born for performance improvemant but when it meets with scaling requirement, things are getting interesting. In this post and upcoming posts, I will describe couple caching strategies and try to compare them in a distributed environment.&lt;/p&gt;

&lt;h4&gt;Strategies&lt;/h4&gt;

&lt;p&gt;In this section, I will describe 5 types of caching strategies: &lt;em&gt;cache aside&lt;/em&gt;, &lt;em&gt;read through&lt;/em&gt;, &lt;em&gt;read ahead&lt;/em&gt;, &lt;em&gt;write through&lt;/em&gt;, &lt;em&gt;write behind&lt;/em&gt;. &lt;/p&gt;

&lt;h5&gt;cache aside&lt;/h5&gt;

&lt;p&gt;This is where application is responsible for reading and writing from the database and the cache doesn&amp;#39;t interact with the database at all. The cache is &amp;quot;kept aside&amp;quot; as a faster and more scalable in-memory data store. The application checks the cache before reading anything from the database. And, the application updates the cache after making any updates to the database. This way, the application ensures that the cache is kept synchronized with the database.&lt;/p&gt;

&lt;h5&gt;read through&lt;/h5&gt;

&lt;p&gt;This is where cache store is responsible for reading from the database. When an application asks the cache for certain key and miss, that will delegate cache store to fetch data from database and populate it in cache for future use. And then return data to application. This performance can be improved by &lt;em&gt;read ahead&lt;/em&gt;&lt;/p&gt;

&lt;h5&gt;read ahead&lt;/h5&gt;

&lt;p&gt;This will allow developers to configure a cache to automatically and &lt;em&gt;asynchronously&lt;/em&gt; reload any recently accessed cache entry from the cache loader before its expiration. The result is that after a frequently accessed entry has entered the cache, the application will not fell the impact of a read against a potentially slow cache when the entry is reloaded due to expiration. The &lt;em&gt;asynchronous&lt;/em&gt; refresh is only triggered when a object that is sufficiently close to its expiration time is accessed. &lt;/p&gt;

&lt;h5&gt;write through&lt;/h5&gt;

&lt;p&gt;When the application updates a piece of data in the cache, the operation will not complete/return until the database is also got updated. This does not improve write performance since you are still dealing with the latency of the write to database. Improving write performance is the purpose for the &lt;em&gt;write behind&lt;/em&gt;.&lt;/p&gt;

&lt;h5&gt;write behind&lt;/h5&gt;

&lt;p&gt;In this strategy, modified cache entries are asynchronously written to the data source after a configurable delay, whether after 10 seconds, 20 minutes, a day or even a week or longer.For Write-Behind caching, we will maintains a write-behind queue of the data that must be updated in the data source. When the application updates X in the cache, X is added to the write-behind queue (if it isn&amp;#39;t there already; otherwise, it is replaced), and after the specified write-behind delay we will call the cache to update the underlying database with the latest state of X. Note that the write-behind delay is relative to the first of a series of modifications—in other words, the data in the data source will never lag behind the cache by more than the write-behind delay.&lt;/p&gt;

&lt;h4&gt;Comparison&lt;/h4&gt;

&lt;h5&gt;Read-Through/Write-Through vs. cache-aside&lt;/h5&gt;

&lt;p&gt;There are two common approaches to the cache-aside pattern in a clustered environment. One involves checking for a cache miss, then querying the database, populating the cache, and continuing application processing. This can result in multiple database hits if different application threads perform this processing at the same time. Alternatively, applications may perform double-checked locking (which works since the check is atomic with respect to the cache entry). This, however, results in a substantial amount of overhead on a cache miss or a database update (a clustered lock, additional read, and clustered unlock – up to 10 additional network hops, or 6-8ms on a typical gigabit ethernet connection, plus additional processing overhead and an increase in the &amp;quot;lock duration&amp;quot; for a cache entry).&lt;/p&gt;

&lt;p&gt;By using inline caching, the entry is locked only for the 2 network hops (while the data is copied to the backup server for fault-tolerance). Additionally, the locks are maintained locally on the partition owner. Furthermore, application code is fully managed on the cache server, meaning that only a controlled subset of nodes will directly access the database (resulting in more predictable load and security). Additionally, this decouples cache clients from database logic.&lt;/p&gt;

&lt;h5&gt;Refresh ahead vs. Read Through&lt;/h5&gt;

&lt;p&gt;Refresh-ahead offers reduced latency compared to read-through, but only if the cache can accurately predict which cache items are likely to be needed in the future. With full accuracy in these predictions, refresh-ahead will offer reduced latency and no added overhead. The higher the rate of misprediction, the greater the impact will be on throughput (as more unnecessary requests will be sent to the database) – potentially even having a negative impact on latency should the database start to fall behind on request processing.&lt;/p&gt;

&lt;h5&gt;Write Behind vs. Write Through&lt;/h5&gt;

&lt;p&gt;If the requirements for write-behind caching can be satisfied, write-behind caching may deliver considerably higher throughput and reduced latency compared to write-through caching. Additionally write-behind caching lowers the load on the database (fewer writes), and on the cache server (reduced cache value deserialization).&lt;/p&gt;</content><author><name></name></author><category term="caching" /><summary type="html">Background In current tech world, performance and scaling are 2 main factors when we design a system. Cache is born for performance improvemant but when it meets with scaling requirement, things are getting interesting. In this post and upcoming posts, I will describe couple caching strategies and try to compare them in a distributed environment.</summary></entry><entry><title type="html">Fast Text Cache–Feed Forward Bloom Filter</title><link href="http://localhost:4000/system/2013/09/27/BLOOM.html" rel="alternate" type="text/html" title="Fast Text Cache--Feed Forward Bloom Filter" /><published>2013-09-27T00:00:00-07:00</published><updated>2013-09-27T00:00:00-07:00</updated><id>http://localhost:4000/system/2013/09/27/BLOOM</id><content type="html" xml:base="http://localhost:4000/system/2013/09/27/BLOOM.html">&lt;p&gt;In my current work, I want to speed up the text pattern match and have good cache behavior. For pattern match, there are several pretty good algorithm : &lt;em&gt;KMP&lt;/em&gt; , &lt;em&gt;Rabin Karp&lt;/em&gt;.I found the following algorithm is extremely awesome!!!! This algorithm quickly filter out non-related corpus. It reduces quite a lot in terms of problem size. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.cmu.edu/%7Edga/papers/fastcache-tr.pdf&quot;&gt;white paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Following is my learning note.&lt;/p&gt;

&lt;h4&gt;Bloom Filter&lt;/h4&gt;

&lt;p&gt;Definition from wiki:&lt;br&gt;
An empty Bloom filter is a bit array of m bits, all set to 0. There must also be k different hash functions defined, each of which maps or hashes some set element to one of the m array positions with a uniform random distribution.&lt;em&gt;Probability of false positives&lt;/em&gt; :Assume that a hash function selects each array position with equal probability. If m is the number of bits in the array , and k is the number of the numver of hash functions, then the probability that a certain bit is not set to 1 by a certain hash function during the insertion of an element is then : $1 - \frac{1}{m}$. The probability that it is not set to 1 by any of the hash functions is $(1 - \frac{1}{m})^k$ . If we have inserted n elements, the probability that a certain bit is still 0 is $(1 - \frac{1}{m})^{kn}$. The probability that it is 1 is therefore $1 - (1-\frac{1}{m})^{kn}$. Now test membership of anelement that is not in the set. Each of the k array positions computed by the hash function is 1 with probability as above. The probability of all of them being 1, which would cause the algorithm erroneously claim that the element is in the set, is often given as $(1 - (1 - \frac{1}{m})^{kn})^{k} = (1 - \exp^{-kn/m})^k$.&lt;/p&gt;

&lt;p&gt;So basically, bloom filter has pretty good rejection feature. Only false positives could happen.&lt;/p&gt;

&lt;h4&gt;Feed Forward Bloom Fitlers&lt;/h4&gt;

&lt;p&gt;For &lt;em&gt;exact&lt;/em&gt;match acceleration_, Bloom filters are typically used as a filter before a traditional matching phase. Like a traditional Bloom Filter, the feed-forward Bloom filter reduces the size of the corpus before cleanup. Unlike traditional filters, however, it also uses information determined while filtering the corpus to eliminate many of the patterns from the second phase. As a result, it reduces drastically the memory used for cleanup.&lt;/p&gt;

&lt;h4&gt;Cache-partitioned Bloom Filters&lt;/h4&gt;

&lt;p&gt;A lookup in a typical Bloom filter involves computing a number of hash values for a query, and using these values as indices when accessing a bit vector. Because the hash values must be randomly distributed for the filter to be effective, and since, for millions of pattherns, the bit vector needs to be a lot larger than the cache available on modern CPUs, Bloom filter implementations have poor cache performance. The &lt;em&gt;solution&lt;/em&gt; above paper provided is to split the bloom filter into 2 parts. The first part is smaller than the largest CPU cache available(typically L2 cache) and is the only one accessed for the wide majority of the lookups. In consequence, it will remain entirely cache-resident. The second part of the filter is larger, but is accessed infrequently. The result of this paper is that the cache-partitioned Bloom filter is as effective as the classic Bloom filter, but has much better cache performance.&lt;/p&gt;

&lt;h4&gt;Design and Implementation&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;High Level&lt;/em&gt;
1. a feed-forward Bloom filter(FFBF) is built from the large set of patterns.&lt;br&gt;
2. It is used to scan the corpus and discard every item(e.g. line of text, if the patterns cannot span multiple lines, or input fragment) that does not generate hits in the filter and therefore cannot contain any matches.&lt;br&gt;
3. The set of patterns is then scanned using feed-forward information obtained during the corpus scan. Only those patterns for which there is a chance of a match in the filtered corpus are kept for the next phase.&lt;br&gt;
4. At this point, all that is left to do is search for a small fraction of the initial number of patterns in a small fragment of the text corpus. Therefore, this exact matching step can be performed quickly and with minimal memory requirements using any traditional multiple pattern matching algorithm. Notice that the large set of patterns does not have to be memory -resident at any point during the execution of our algorithm -- we only need to stream it sequentially from external media.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/BLOOM.png&quot; alt=&quot;BLOOM&quot; title=&quot;BLOOM&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;My feeling&lt;/h4&gt;

&lt;p&gt;Bloom filter works pretty well when acting as reducing problem size. That actually reminds me of other applications that will be improved by this approach, especially before we do distributed computing, we can reduce the problem size quite bit if we define a &lt;em&gt;&amp;quot;domain&amp;quot;&lt;/em&gt; bloom filter. &lt;/p&gt;</content><author><name></name></author><category term="caching" /><summary type="html">In my current work, I want to speed up the text pattern match and have good cache behavior. For pattern match, there are several pretty good algorithm : KMP , Rabin Karp.I found the following algorithm is extremely awesome!!!! This algorithm quickly filter out non-related corpus. It reduces quite a lot in terms of problem size.</summary></entry><entry><title type="html">Data Infrastructure - Graph Traversal Pattern II</title><link href="http://localhost:4000/system/2013/08/17/GTP2.html" rel="alternate" type="text/html" title="Data Infrastructure - Graph Traversal Pattern II" /><published>2013-08-17T00:00:00-07:00</published><updated>2013-08-17T00:00:00-07:00</updated><id>http://localhost:4000/system/2013/08/17/GTP2</id><content type="html" xml:base="http://localhost:4000/system/2013/08/17/GTP2.html">&lt;h4&gt;Graph Traversals&lt;/h4&gt;

&lt;p&gt;A traversal means visting elements (vertices and edges) in a graph in some algorithmic fashion. This subsection will introduce a functional flowbased approach to traversing property graphs and how different types of traversals over different types of graph datasets support different types of problem-solving.&lt;/p&gt;

&lt;p&gt;The most primitive, read-based operation on a graph is a single step traversal from element $i$ to element $j$, where $i,j \in (V \cup E)$. For example, &amp;quot;Find the outgoing edges from this vertex&amp;quot;,&amp;quot;Find the vertex that is at the end of this edge&amp;quot;. Single step operations expose explicit adjacencies in the graph. Following list itemizes the various types of single step traversals. &lt;/p&gt;

&lt;p&gt;$e&lt;em&gt;{out}$ : traverse to the outgoing edges of the vertices&lt;br&gt;
$e&lt;/em&gt;{in}$ :traverse to the incoming edges to the vertices&lt;br&gt;
$v&lt;em&gt;{out}$ : traverse to the outgoing (or say tail) vertices of the edges&lt;br&gt;
$v&lt;/em&gt;{in}$ : traverse the incoming(or say head) vertices of the edges&lt;br&gt;
$\epsilon$ : get the element property values for given key      &lt;/p&gt;

&lt;p&gt;When edges are labeled and elements have properties, it is desirable to constrain the traversal to edges of a particular label or elements with particular label or elements with particular properties. These operations are known as filters and are abstractly defined as following:&lt;/p&gt;

&lt;p&gt;$e&lt;em&gt;{lab+}$ allow all edges with label&lt;br&gt;
$e&lt;/em&gt;{lab-}$ filter all edges with label&lt;br&gt;
$\epsilon&lt;em&gt;{p+}$ allow all elements with the property&lt;br&gt;
$\epsilon&lt;/em&gt;{p-}$ filter all elements with the property&lt;br&gt;
$\epsilon&lt;em&gt;{\epsilon +}$ allow all elements that are the provided element&lt;br&gt;
$\epsilon&lt;/em&gt;{\epsilon -}$ filter all elements that are the provided element     &lt;/p&gt;

&lt;p&gt;A simple example is traversing to the names of Alberto Pepe&amp;#39;s friends of last post. If $i$ is the vertex representing Alberto Pepe.&lt;/p&gt;

&lt;p&gt;$f(i) = \epsilon(v&lt;em&gt;{int}(e&lt;/em&gt;{lab+}(e_{out}(i),friend)), name)$&lt;/p&gt;

&lt;p&gt;then $f(i)$ will return the names of Alberto Pepe&amp;#39;s friends. By function currying and composition, we can rewrite the above definition.&lt;/p&gt;

&lt;p&gt;$f(i) = (\epsilon^{name} \cdot v&lt;em&gt;{in} \cdot e^{friend}&lt;/em&gt;{lab+} \cdot e_{out})(i)$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GTP5.png&quot; alt=&quot;GTP5&quot; title=&quot;GTP5&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Traversal Pattern of Recommendation&lt;/h4&gt;

&lt;p&gt;Like I mentioned in last post, currently every giant tech company has some sort of graph data. In this stage, every company want to do some sort of recommendation to increase user engagement.So let us focus on traversal pattern of recommendation. Following is how the graph looks like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GTP6.png&quot; alt=&quot;GTP6&quot; title=&quot;GTP6&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h5&gt;Content-Based Recommendation&lt;/h5&gt;

&lt;p&gt;In order to identify resources that are similar in features to a resource, traverse to all resources that share the same features.  Here is the traversal pattern function definition. Let $i$ be the resource&lt;/p&gt;

&lt;p&gt;$f(i) = (\epsilon^{i}&lt;em&gt;{\epsilon -} \cdot v&lt;/em&gt;{out} \cdot e^{feature}&lt;em&gt;{lab+} \cdot e&lt;/em&gt;{in} \cdot v&lt;em&gt;{in} \cdot e^{feature}&lt;/em&gt;{lab+} \cdot e_{out}) (i)$&lt;/p&gt;

&lt;p&gt;Following figure describe the processing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GTP7.png&quot; alt=&quot;GTP7&quot; title=&quot;GTP7&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is one &lt;em&gt;important&lt;/em&gt; extension to this problem : &amp;quot;Given what person $i$ likes, what other resources have similar features?&amp;quot;  We just need to define an additinal function $g$ of person $i$&lt;/p&gt;

&lt;p&gt;$g(i) = (v&lt;em&gt;{in} \cdot e^{likes}&lt;/em&gt;{lab+} \cdot e_{out}) (i)$&lt;/p&gt;

&lt;p&gt;and then our problem can be solved by $f \dot g$&lt;/p&gt;

&lt;h5&gt;Collaborative Filtering-Based Recommendation&lt;/h5&gt;

&lt;p&gt;In the world of recommendation, there is another recommendation called &amp;quot;collaborative filtering based recommendation&amp;quot;. With collaborative filtering, the objective is to identify a set of resources that have a high probability of being liked by a person based upon identifying other people in the system that enjoy similar likes. For exampl, if person $a$ and person $b$ share 90% of their liked resources in common, then the remaining 10% they don&amp;#39;t share in common are candidates for recommendation. Solving the problem of collaborative filtering using graph traversals can be accomplished 2 components.&lt;/p&gt;

&lt;p&gt;$f(i) = (\epsilon^{i}&lt;em&gt;{\epsilon -} \cdot v&lt;/em&gt;{out} \cdot e^{like}&lt;em&gt;{lab+} \cdot e&lt;/em&gt;{in} \cdot v&lt;em&gt;{in} \cdot e^{like}&lt;/em&gt;{lab+} \cdot e_{out}) (i)$&lt;/p&gt;

&lt;p&gt;Function $f$ traverses to all those people vertices that like the same resources as person vertex $i$ and who themselves are not vertex $i$.&lt;/p&gt;

&lt;p&gt;$g(j) = (v&lt;em&gt;{in} \cdot e^{like}&lt;/em&gt;{lab+} \cdot e_{out}) (j)$&lt;/p&gt;

&lt;p&gt;Fucntion $g$ traverses to all the resources liked by vertex $j$. In composition, $(g \cdot f)(i)$ determines all those resources that are liked by those people that have similar tastes to vertex $i$. If person $j$ likes 10 resources in common with person $i$, then the resources that person likes will be returned at least 10 times by $g \cdot f$ (perhaps more if a path exists to those resources from another person vertex as well)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GTP8.png&quot; alt=&quot;GTP8&quot; title=&quot;GTP8&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;With the graph traversal pattern, there exists a single graph data structure that can be traversed in different ways to expose different types of recommendations|generally, different types of relationships between vertices.Being able to mix and match the types of traversals executed alters the semantics of the final rankings and conveniently allows for hybrid recommendation algorithms to emerge.&lt;/p&gt;</content><author><name></name></author><category term="Recommendation Traversal Pattern" /><summary type="html">Graph Traversals A traversal means visting elements (vertices and edges) in a graph in some algorithmic fashion. This subsection will introduce a functional flowbased approach to traversing property graphs and how different types of traversals over different types of graph datasets support different types of problem-solving. The most primitive, read-based operation on a graph is a single step traversal from element $i$ to element $j$, where $i,j \in (V \cup E)$. For example, &amp;quot;Find the outgoing edges from this vertex&amp;quot;,&amp;quot;Find the vertex that is at the end of this edge&amp;quot;. Single step operations expose explicit adjacencies in the graph. Following list itemizes the various types of single step traversals. $e{out}$ : traverse to the outgoing edges of the vertices $e{in}$ :traverse to the incoming edges to the vertices $v{out}$ : traverse to the outgoing (or say tail) vertices of the edges $v{in}$ : traverse the incoming(or say head) vertices of the edges $\epsilon$ : get the element property values for given key When edges are labeled and elements have properties, it is desirable to constrain the traversal to edges of a particular label or elements with particular label or elements with particular properties. These operations are known as filters and are abstractly defined as following: $e{lab+}$ allow all edges with label $e{lab-}$ filter all edges with label $\epsilon{p+}$ allow all elements with the property $\epsilon{p-}$ filter all elements with the property $\epsilon{\epsilon +}$ allow all elements that are the provided element $\epsilon{\epsilon -}$ filter all elements that are the provided element A simple example is traversing to the names of Alberto Pepe&amp;#39;s friends of last post. If $i$ is the vertex representing Alberto Pepe. $f(i) = \epsilon(v{int}(e{lab+}(e_{out}(i),friend)), name)$ then $f(i)$ will return the names of Alberto Pepe&amp;#39;s friends. By function currying and composition, we can rewrite the above definition. $f(i) = (\epsilon^{name} \cdot v{in} \cdot e^{friend}{lab+} \cdot e_{out})(i)$ Traversal Pattern of Recommendation Like I mentioned in last post, currently every giant tech company has some sort of graph data. In this stage, every company want to do some sort of recommendation to increase user engagement.So let us focus on traversal pattern of recommendation. Following is how the graph looks like. Content-Based Recommendation In order to identify resources that are similar in features to a resource, traverse to all resources that share the same features. Here is the traversal pattern function definition. Let $i$ be the resource $f(i) = (\epsilon^{i}{\epsilon -} \cdot v{out} \cdot e^{feature}{lab+} \cdot e{in} \cdot v{in} \cdot e^{feature}{lab+} \cdot e_{out}) (i)$ Following figure describe the processing. There is one important extension to this problem : &amp;quot;Given what person $i$ likes, what other resources have similar features?&amp;quot; We just need to define an additinal function $g$ of person $i$ $g(i) = (v{in} \cdot e^{likes}{lab+} \cdot e_{out}) (i)$ and then our problem can be solved by $f \dot g$ Collaborative Filtering-Based Recommendation In the world of recommendation, there is another recommendation called &amp;quot;collaborative filtering based recommendation&amp;quot;. With collaborative filtering, the objective is to identify a set of resources that have a high probability of being liked by a person based upon identifying other people in the system that enjoy similar likes. For exampl, if person $a$ and person $b$ share 90% of their liked resources in common, then the remaining 10% they don&amp;#39;t share in common are candidates for recommendation. Solving the problem of collaborative filtering using graph traversals can be accomplished 2 components. $f(i) = (\epsilon^{i}{\epsilon -} \cdot v{out} \cdot e^{like}{lab+} \cdot e{in} \cdot v{in} \cdot e^{like}{lab+} \cdot e_{out}) (i)$ Function $f$ traverses to all those people vertices that like the same resources as person vertex $i$ and who themselves are not vertex $i$. $g(j) = (v{in} \cdot e^{like}{lab+} \cdot e_{out}) (j)$ Fucntion $g$ traverses to all the resources liked by vertex $j$. In composition, $(g \cdot f)(i)$ determines all those resources that are liked by those people that have similar tastes to vertex $i$. If person $j$ likes 10 resources in common with person $i$, then the resources that person likes will be returned at least 10 times by $g \cdot f$ (perhaps more if a path exists to those resources from another person vertex as well) Conclusion With the graph traversal pattern, there exists a single graph data structure that can be traversed in different ways to expose different types of recommendations|generally, different types of relationships between vertices.Being able to mix and match the types of traversals executed alters the semantics of the final rankings and conveniently allows for hybrid recommendation algorithms to emerge.</summary></entry><entry><title type="html">Data Infrastructure - Graph Traversal Pattern I</title><link href="http://localhost:4000/system/2013/08/13/GTP1.html" rel="alternate" type="text/html" title="Data Infrastructure - Graph Traversal Pattern I" /><published>2013-08-13T00:00:00-07:00</published><updated>2013-08-13T00:00:00-07:00</updated><id>http://localhost:4000/system/2013/08/13/GTP1</id><content type="html" xml:base="http://localhost:4000/system/2013/08/13/GTP1.html">&lt;p&gt;I found a very good paper which describe the graph traversal pattern and its underlying infrastructure to go.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://arxiv.org/pdf/1004.1001v1.pdf&quot;&gt;The Graph Traversal Pattern&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;Introduction&lt;/h4&gt;

&lt;p&gt;Today, every tech company talks about big data. But what exactly those companies&amp;#39; data look like? Take Facebook as an example. Facebook owns a social network data which take people as nodes and friendships as edges. Facebook is a People Graph. For Google, it takes web documents as nodes and hyperlinks as edges. The big data Google has is a Knowledge Graph. For Pinterest, it takes 2 nodes: people and pin boards, takes interests as edges.&lt;/p&gt;

&lt;p&gt;With all these examples, we can find theoretically they are all &amp;quot;Property Graphs&amp;quot; with attributed edges. This series of posts will discuss topics about the data infrastructure and the common operation(purpose) on top of the infrastructure. Basically, the most common operation is &amp;quot;Recommendation&amp;quot;.&lt;/p&gt;

&lt;h4&gt;Constraint of Relational Database&lt;/h4&gt;

&lt;p&gt;Relational databases maintain a collection of tables. Each table can be defined by a set of rows and a set of columns. Traditionally, rows denote objects and columns denote properties/attributes. Usually, a problem domain is modelled over multiple tables in order to avoid data duplication. This process is known as &lt;em&gt;data normalization&lt;/em&gt; . In order to unify data in separate tables, a &lt;em&gt;&amp;quot;join&amp;quot;&lt;/em&gt; is used. A join combines two tables when columns of one table refer to columns of another table. Maintaining these references in a consistent state is know as a referential integrity. &lt;/p&gt;

&lt;p&gt;An example described in that paper: we have 2 tables. One is people table and another is friendship table. We want to get all friends of Alberto Pepe.Usually, relational database has tree-structured index built on top of table.(Like B+ tree), which can enable log n query complexity.     &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GTP2.png&quot; alt=&quot;GTP2&quot; title=&quot;GTP2&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GTP1.png&quot; alt=&quot;GTP1&quot; title=&quot;GTP1&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The final operation yields the names of Alberto&amp;#39;s friends. This example elucidates the classic join operation utilized in relational databases. By being able to join the person and friend table, its possible to move from a name efffect , to the person, to his or her friends, and then finally to their names. In effect, the join operation forms a &lt;em&gt;graph&lt;/em&gt; that is dynamically constructed as one table is linked to another table. The limitation is that this graph is not explicit in the relational structure , but instead must be inferred through a series of index-intensive operations. Moreover, while only a particular subset of the data in the database may be desired, all data in all queried tables must be examined in order to extract the desired subset. &lt;/p&gt;

&lt;p&gt;The $log(n)$ read-time is fast for a search, as the index grow larger with the growth of the data and as more join operations are used, this model becomes inefficient. At the limit, the inferred graph that is constructed through joins is best solved, &lt;em&gt;graph database&lt;/em&gt; .   &lt;/p&gt;

&lt;h4&gt;The Graph as an Index&lt;/h4&gt;

&lt;p&gt;Most of graph theory is concerned with the development of theorems for single relational graphs. A single-relational graph maintains a set of edges, where all the edges are homogeneous. For example, all edges denote friendship or kinship, but not both together within the same structure. In application, complex domain models are more conveniently represented by multi-relational, &lt;em&gt;property graphs.&lt;/em&gt; The edges in a property graph are typed or labeled and thus, edges are heterogenous. For example, a property graph can model friendship, kinship, business, communication,etc. relationships all within the same structure. Moreover, vertices and edges in a property graph maintain a set of key/value pairs. These are known as properties and allow for the representation of non-graph data,like the name of a vertex, the weight of an edge. Formally, a property graph can be defined as $G= (V,E,\lambda,\mu)$, where edges are directed, edges are labeled($\lambda$ : $E \rightarrow \sum$), and properties are a map from elements and keys to values.&lt;/p&gt;

&lt;p&gt;Now let&amp;#39;s see the &lt;em&gt;graph database&lt;/em&gt; apporach in previous example.     &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GTP3.png&quot; alt=&quot;GTP3&quot; title=&quot;GTP3&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/GTP4.png&quot; alt=&quot;GTP4&quot; title=&quot;GTP4&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In a graph, database, there is no explicit join operation because vertices maintain direct references to their adjacent edges. In many ways, the edges of the graph serve as explicit, &amp;quot;hard-wired&amp;quot; join structures. The act of traversing over an edge is the act of joining. What makes this more efficient in a graph database is that traversing from one vertex to another is a constant time operation.&lt;/p&gt;</content><author><name></name></author><category term="Relational_Database Graph_Index" /><summary type="html">I found a very good paper which describe the graph traversal pattern and its underlying infrastructure to go. The Graph Traversal Pattern Introduction Today, every tech company talks about big data. But what exactly those companies&amp;#39; data look like? Take Facebook as an example. Facebook owns a social network data which take people as nodes and friendships as edges. Facebook is a People Graph. For Google, it takes web documents as nodes and hyperlinks as edges. The big data Google has is a Knowledge Graph. For Pinterest, it takes 2 nodes: people and pin boards, takes interests as edges. With all these examples, we can find theoretically they are all &amp;quot;Property Graphs&amp;quot; with attributed edges. This series of posts will discuss topics about the data infrastructure and the common operation(purpose) on top of the infrastructure. Basically, the most common operation is &amp;quot;Recommendation&amp;quot;. Constraint of Relational Database Relational databases maintain a collection of tables. Each table can be defined by a set of rows and a set of columns. Traditionally, rows denote objects and columns denote properties/attributes. Usually, a problem domain is modelled over multiple tables in order to avoid data duplication. This process is known as data normalization . In order to unify data in separate tables, a &amp;quot;join&amp;quot; is used. A join combines two tables when columns of one table refer to columns of another table. Maintaining these references in a consistent state is know as a referential integrity. An example described in that paper: we have 2 tables. One is people table and another is friendship table. We want to get all friends of Alberto Pepe.Usually, relational database has tree-structured index built on top of table.(Like B+ tree), which can enable log n query complexity. The final operation yields the names of Alberto&amp;#39;s friends. This example elucidates the classic join operation utilized in relational databases. By being able to join the person and friend table, its possible to move from a name efffect , to the person, to his or her friends, and then finally to their names. In effect, the join operation forms a graph that is dynamically constructed as one table is linked to another table. The limitation is that this graph is not explicit in the relational structure , but instead must be inferred through a series of index-intensive operations. Moreover, while only a particular subset of the data in the database may be desired, all data in all queried tables must be examined in order to extract the desired subset. The $log(n)$ read-time is fast for a search, as the index grow larger with the growth of the data and as more join operations are used, this model becomes inefficient. At the limit, the inferred graph that is constructed through joins is best solved, graph database . The Graph as an Index Most of graph theory is concerned with the development of theorems for single relational graphs. A single-relational graph maintains a set of edges, where all the edges are homogeneous. For example, all edges denote friendship or kinship, but not both together within the same structure. In application, complex domain models are more conveniently represented by multi-relational, property graphs. The edges in a property graph are typed or labeled and thus, edges are heterogenous. For example, a property graph can model friendship, kinship, business, communication,etc. relationships all within the same structure. Moreover, vertices and edges in a property graph maintain a set of key/value pairs. These are known as properties and allow for the representation of non-graph data,like the name of a vertex, the weight of an edge. Formally, a property graph can be defined as $G= (V,E,\lambda,\mu)$, where edges are directed, edges are labeled($\lambda$ : $E \rightarrow \sum$), and properties are a map from elements and keys to values. Now let&amp;#39;s see the graph database apporach in previous example. In a graph, database, there is no explicit join operation because vertices maintain direct references to their adjacent edges. In many ways, the edges of the graph serve as explicit, &amp;quot;hard-wired&amp;quot; join structures. The act of traversing over an edge is the act of joining. What makes this more efficient in a graph database is that traversing from one vertex to another is a constant time operation.</summary></entry><entry><title type="html">Lessons from Memcached</title><link href="http://localhost:4000/system/2013/08/08/mem.html" rel="alternate" type="text/html" title="Lessons from Memcached" /><published>2013-08-08T00:00:00-07:00</published><updated>2013-08-08T00:00:00-07:00</updated><id>http://localhost:4000/system/2013/08/08/mem</id><content type="html" xml:base="http://localhost:4000/system/2013/08/08/mem.html">&lt;p&gt;This week I spent some time on reading source code of memcached &lt;a href=&quot;https://github.com/memcached/memcached&quot;&gt;GitHub Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Followings are some lessons.&lt;/p&gt;

&lt;h5&gt;Memory Allocattion&lt;/h5&gt;

&lt;p&gt;The memcached server allocates memory by using &amp;quot;Slab allocator&amp;quot;. The reason why this slab allocator is used over malloc/free is to avoid &lt;em&gt;fragmentation&lt;/em&gt; and the operating system having to spend cycles searching for contiguous blocks of memory. These tasks overall, tend to consume more resources than the memcached process itself. With the slab allocator, memory is allocated in chunks and in turn, is constantly being reused. Because memory is allocated into different size slabs, it will waste memory at some extent if the data being cached does not fit perfectly into the slab. Following is the struct definition of slab, which tells us the size is in powers of N. In current implementation, the largest slab size is 1 MB, which means cached data cannot exceed this size.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;&lt;span class=&quot;cm&quot;&gt;/* powers-of-N allocation structures */&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;typedef&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;      &lt;span class=&quot;cm&quot;&gt;/* sizes of items */&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;perslab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;   &lt;span class=&quot;cm&quot;&gt;/* how many items per slab */&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;           &lt;span class=&quot;cm&quot;&gt;/* list of item ptrs */&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sl_curr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;   &lt;span class=&quot;cm&quot;&gt;/* total free items in list */&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slabs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;     &lt;span class=&quot;cm&quot;&gt;/* how many slabs were allocated for this class */&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slab_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;       &lt;span class=&quot;cm&quot;&gt;/* array of slab pointers */&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* size of prev array */&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;killing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;  &lt;span class=&quot;cm&quot;&gt;/* index+1 of dying slab, or zero if none */&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requested&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* The number of requested bytes */&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slabclass_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h5&gt;Replacement Strategy&lt;/h5&gt;

&lt;p&gt;When memcached&amp;#39;s distributed hash table becomes full, the upcoming inserts force older cached data to be cycled out in a LRU order with associated expiration timeouts. Memcached uses &lt;em&gt;lazy expiration&lt;/em&gt;. This menas it does not make use of additional CPU cycles to expire items. When data is requestd via a &amp;quot;get&amp;quot; request, memcached references the expiration time to confirm if the data is valid before returnning it to the client requesting the data. When new data is being added to the cache via a &amp;quot;set&amp;quot;, and memcached is unable to allocate an additional slab, expired data will be cycled out prior to any data that qualifies for the LRU criteria.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;&lt;span class=&quot;cm&quot;&gt;/*
 * Stores an item in the cache according to the semantics of one of the set
 * commands. In threaded mode, this is protected by the cache lock.
 *
 * Returns the state of storage.
 */&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;enum&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;store_item_type&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;do_store_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ITEM_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;do_item_get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nkey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;enum&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;store_item_type&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NOT_STORED&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;NULL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NREAD_ADD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;cm&quot;&gt;/* add only adds a nonexistent item, but promote to head of LRU */&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;do_item_update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NREAD_REPLACE&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NREAD_APPEND&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NREAD_PREPEND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;cm&quot;&gt;/* replace only replaces an existing value; don't store */&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NREAD_CAS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;cm&quot;&gt;/* validate cas operation */&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;// LRU expired
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NOT_FOUND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pthread_mutex_lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cas_misses&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pthread_mutex_unlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ITEM_get_cas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ITEM_get_cas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;// cas validates
&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;// it and old_it may belong to different classes.
&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;// I'm updating the stats for the one that's getting pushed out
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;pthread_mutex_lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slab_stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slabs_clsid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cas_hits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pthread_mutex_unlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;item_replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;STORED&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pthread_mutex_lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slab_stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slabs_clsid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cas_badval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pthread_mutex_unlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;settings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;fprintf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stderr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;CAS:  failure: expected %llu, got %llu&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ITEM_get_cas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ITEM_get_cas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EXISTS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;cm&quot;&gt;/*
         * Append - combine new and old record into single one. Here it's
         * atomic and thread-safe.
         */&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NREAD_APPEND&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NREAD_PREPEND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;cm&quot;&gt;/*
             * Validate CAS
             */&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ITEM_get_cas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;// CAS much be equal
&lt;/span&gt;                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ITEM_get_cas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ITEM_get_cas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EXISTS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NOT_STORED&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;cm&quot;&gt;/* we have it and old_it here - alloc memory to hold both */&lt;/span&gt;
                &lt;span class=&quot;cm&quot;&gt;/* flags was already lost - so recover them from ITEM_suffix(it) */&lt;/span&gt;

                &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strtol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ITEM_suffix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

                &lt;span class=&quot;n&quot;&gt;new_it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;do_item_alloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nkey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exptime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbytes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbytes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* CRLF */&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;cm&quot;&gt;/* SERVER_ERROR out of memory */&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;do_item_remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

                    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NOT_STORED&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

                &lt;span class=&quot;cm&quot;&gt;/* copy data from it and old_it to new_it */&lt;/span&gt;

                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NREAD_APPEND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;memcpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ITEM_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ITEM_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;memcpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ITEM_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbytes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* CRLF */&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ITEM_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;cm&quot;&gt;/* NREAD_PREPEND */&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;memcpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ITEM_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ITEM_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;memcpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ITEM_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbytes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* CRLF */&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ITEM_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbytes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

                &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NOT_STORED&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;item_replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;do_item_link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ITEM_get_cas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;STORED&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;do_item_remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;         &lt;span class=&quot;cm&quot;&gt;/* release our reference */&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_it&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;do_item_remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;STORED&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ITEM_get_cas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stored&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h5&gt;Data Redundancy and Reproduce&lt;/h5&gt;

&lt;p&gt;By design, there are no data redundancy features built into memcached. Memcached is designed to be a scalable and high performance caching-layer, including data redundancy functionality would only add complexity and overhead to the system.&lt;/p&gt;

&lt;p&gt;If memcached servers suffer from a loss of data, it should still be able to retrieve its data from the original source database. &lt;/p&gt;

&lt;p&gt;Another thing is memcached does not have any built in reproduce operation when first attempt fails. One can simply have an abundance of nodes. Because memcached is designed to scale straight out-of-the-box, this is a key characteristic to exploit. Having plenty of memcached nodes minimizes the overall impact an outage of one or more nodes will have on the system as a whole.&lt;/p&gt;

&lt;h5&gt;Loading Memcached&lt;/h5&gt;

&lt;p&gt;In general, &amp;quot;warming-up&amp;quot; memcached from a database dump is not the best course of 
action to take. Several issues arise with this method. First, changes to data that have 
occurred between the data dump and load will not be accounted for. Second, there must be 
some strategy for dealing with data that may have expired prior to the dump being loaded. &lt;/p&gt;

&lt;p&gt;In situations where there are large amounts of fairly static or permanent data to be cached, 
using a data dump/load can be useful for warming up the cache quickly. &lt;/p&gt;</content><author><name></name></author><category term="Memcached" /><summary type="html">This week I spent some time on reading source code of memcached GitHub Link Followings are some lessons. Memory Allocattion The memcached server allocates memory by using &amp;quot;Slab allocator&amp;quot;. The reason why this slab allocator is used over malloc/free is to avoid fragmentation and the operating system having to spend cycles searching for contiguous blocks of memory. These tasks overall, tend to consume more resources than the memcached process itself. With the slab allocator, memory is allocated in chunks and in turn, is constantly being reused. Because memory is allocated into different size slabs, it will waste memory at some extent if the data being cached does not fit perfectly into the slab. Following is the struct definition of slab, which tells us the size is in powers of N. In current implementation, the largest slab size is 1 MB, which means cached data cannot exceed this size. /* powers-of-N allocation structures */ typedef struct { unsigned int size; /* sizes of items */ unsigned int perslab; /* how many items per slab */ void *slots; /* list of item ptrs */ unsigned int sl_curr; /* total free items in list */ unsigned int slabs; /* how many slabs were allocated for this class */ void **slab_list; /* array of slab pointers */ unsigned int list_size; /* size of prev array */ unsigned int killing; /* index+1 of dying slab, or zero if none */ size_t requested; /* The number of requested bytes */ } slabclass_t; Replacement Strategy When memcached&amp;#39;s distributed hash table becomes full, the upcoming inserts force older cached data to be cycled out in a LRU order with associated expiration timeouts. Memcached uses lazy expiration. This menas it does not make use of additional CPU cycles to expire items. When data is requestd via a &amp;quot;get&amp;quot; request, memcached references the expiration time to confirm if the data is valid before returnning it to the client requesting the data. When new data is being added to the cache via a &amp;quot;set&amp;quot;, and memcached is unable to allocate an additional slab, expired data will be cycled out prior to any data that qualifies for the LRU criteria. /* * Stores an item in the cache according to the semantics of one of the set * commands. In threaded mode, this is protected by the cache lock. * * Returns the state of storage. */ enum store_item_type do_store_item(item *it, int comm, conn *c, const uint32_t hv) { char *key = ITEM_key(it); item *old_it = do_item_get(key, it-&amp;gt;nkey, hv); enum store_item_type stored = NOT_STORED; item *new_it = NULL; int flags; if (old_it != NULL &amp;amp;&amp;amp; comm == NREAD_ADD) { /* add only adds a nonexistent item, but promote to head of LRU */ do_item_update(old_it); } else if (!old_it &amp;amp;&amp;amp; (comm == NREAD_REPLACE || comm == NREAD_APPEND || comm == NREAD_PREPEND)) { /* replace only replaces an existing value; don't store */ } else if (comm == NREAD_CAS) { /* validate cas operation */ if(old_it == NULL) { // LRU expired stored = NOT_FOUND; pthread_mutex_lock(&amp;amp;c-&amp;gt;thread-&amp;gt;stats.mutex); c-&amp;gt;thread-&amp;gt;stats.cas_misses++; pthread_mutex_unlock(&amp;amp;c-&amp;gt;thread-&amp;gt;stats.mutex); } else if (ITEM_get_cas(it) == ITEM_get_cas(old_it)) { // cas validates // it and old_it may belong to different classes. // I'm updating the stats for the one that's getting pushed out pthread_mutex_lock(&amp;amp;c-&amp;gt;thread-&amp;gt;stats.mutex); c-&amp;gt;thread-&amp;gt;stats.slab_stats[old_it-&amp;gt;slabs_clsid].cas_hits++; pthread_mutex_unlock(&amp;amp;c-&amp;gt;thread-&amp;gt;stats.mutex); item_replace(old_it, it, hv); stored = STORED; } else { pthread_mutex_lock(&amp;amp;c-&amp;gt;thread-&amp;gt;stats.mutex); c-&amp;gt;thread-&amp;gt;stats.slab_stats[old_it-&amp;gt;slabs_clsid].cas_badval++; pthread_mutex_unlock(&amp;amp;c-&amp;gt;thread-&amp;gt;stats.mutex); if(settings.verbose &amp;gt; 1) { fprintf(stderr, &quot;CAS: failure: expected %llu, got %llu\n&quot;, (unsigned long long)ITEM_get_cas(old_it), (unsigned long long)ITEM_get_cas(it)); } stored = EXISTS; } } else { /* * Append - combine new and old record into single one. Here it's * atomic and thread-safe. */ if (comm == NREAD_APPEND || comm == NREAD_PREPEND) { /* * Validate CAS */ if (ITEM_get_cas(it) != 0) { // CAS much be equal if (ITEM_get_cas(it) != ITEM_get_cas(old_it)) { stored = EXISTS; } } if (stored == NOT_STORED) { /* we have it and old_it here - alloc memory to hold both */ /* flags was already lost - so recover them from ITEM_suffix(it) */ flags = (int) strtol(ITEM_suffix(old_it), (char **) NULL, 10); new_it = do_item_alloc(key, it-&amp;gt;nkey, flags, old_it-&amp;gt;exptime, it-&amp;gt;nbytes + old_it-&amp;gt;nbytes - 2 /* CRLF */, hv); if (new_it == NULL) { /* SERVER_ERROR out of memory */ if (old_it != NULL) do_item_remove(old_it); return NOT_STORED; } /* copy data from it and old_it to new_it */ if (comm == NREAD_APPEND) { memcpy(ITEM_data(new_it), ITEM_data(old_it), old_it-&amp;gt;nbytes); memcpy(ITEM_data(new_it) + old_it-&amp;gt;nbytes - 2 /* CRLF */, ITEM_data(it), it-&amp;gt;nbytes); } else { /* NREAD_PREPEND */ memcpy(ITEM_data(new_it), ITEM_data(it), it-&amp;gt;nbytes); memcpy(ITEM_data(new_it) + it-&amp;gt;nbytes - 2 /* CRLF */, ITEM_data(old_it), old_it-&amp;gt;nbytes); } it = new_it; } } if (stored == NOT_STORED) { if (old_it != NULL) item_replace(old_it, it, hv); else do_item_link(it, hv); c-&amp;gt;cas = ITEM_get_cas(it); stored = STORED; } } if (old_it != NULL) do_item_remove(old_it); /* release our reference */ if (new_it != NULL) do_item_remove(new_it); if (stored == STORED) { c-&amp;gt;cas = ITEM_get_cas(it); } return stored; } Data Redundancy and Reproduce By design, there are no data redundancy features built into memcached. Memcached is designed to be a scalable and high performance caching-layer, including data redundancy functionality would only add complexity and overhead to the system. If memcached servers suffer from a loss of data, it should still be able to retrieve its data from the original source database. Another thing is memcached does not have any built in reproduce operation when first attempt fails. One can simply have an abundance of nodes. Because memcached is designed to scale straight out-of-the-box, this is a key characteristic to exploit. Having plenty of memcached nodes minimizes the overall impact an outage of one or more nodes will have on the system as a whole. Loading Memcached In general, &amp;quot;warming-up&amp;quot; memcached from a database dump is not the best course of action to take. Several issues arise with this method. First, changes to data that have occurred between the data dump and load will not be accounted for. Second, there must be some strategy for dealing with data that may have expired prior to the dump being loaded. In situations where there are large amounts of fairly static or permanent data to be cached, using a data dump/load can be useful for warming up the cache quickly.</summary></entry><entry><title type="html">Minimal Superpermutation Problem</title><link href="http://localhost:4000/algorithm/2013/07/30/MSP.html" rel="alternate" type="text/html" title="Minimal Superpermutation Problem" /><published>2013-07-30T00:00:00-07:00</published><updated>2013-07-30T00:00:00-07:00</updated><id>http://localhost:4000/algorithm/2013/07/30/MSP</id><content type="html" xml:base="http://localhost:4000/algorithm/2013/07/30/MSP.html">&lt;h4&gt;Question Description&lt;/h4&gt;

&lt;p&gt;We have $1,2,\ldots,n$ symbols. We want to find a shoretest possible string on the symbols that contains every permutation of those symbols as a contiguous substring. We call a string that contains every permutation in this way a $superpermutation$, and one of minimal length is called a $minimal$ $superpermutation$.&lt;/p&gt;

&lt;p&gt;For example, when $n=3$, all permutations are : $123$,$132$,$312$,$213$,$231$,$321$. In this case, the minimal superpermutation is $123121321$. The minimal length is 9.&lt;/p&gt;

&lt;h4&gt;Minimal Length&lt;/h4&gt;

&lt;p&gt;An apparent lower bound of the length of superpermutation on $n$ symbols is $n ! +n-1$, because it must contain every permutation of the $n!$ permutations as a substring - the first permutation has $n$ characters to the string, and each of the remaining $n ! - 1$ permutations have a lengh of at least 1 character more.  A trivial upper bound on the length of a minimal superpermutation is $ n* n !$       &lt;/p&gt;

&lt;p&gt;Suppose we already have a small superpermutation on $n$ and we want to construct a small superpermutation on $n+1$ symbols. To do so, simply replace each permutaiton in the $n$ symbol superpermutation by (1) the permutaiton, (2)the symbol $n+1$ (3) that permutaiton again. &lt;/p&gt;

&lt;p&gt;You can find detail here: &lt;a href=&quot;http://www.notatt.com/permutations.pdf&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;Uniqueness&lt;/h4&gt;

&lt;p&gt;It turns out that there are in fact many superpermutations of the conjectured minimal lengh. The result of [1] shows that there are at least&lt;/p&gt;

&lt;p&gt;$\prod_{k=1}^{n-4} (n-k-2)^{k\cdot k!}$&lt;/p&gt;

&lt;h4&gt;Reference&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0012365X1300157X&quot;&gt;Non-uniqueness of minimal superpermutation&lt;/a&gt; &lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><category term="Superpermutation" /><summary type="html">Question Description We have $1,2,\ldots,n$ symbols. We want to find a shoretest possible string on the symbols that contains every permutation of those symbols as a contiguous substring. We call a string that contains every permutation in this way a $superpermutation$, and one of minimal length is called a $minimal$ $superpermutation$. For example, when $n=3$, all permutations are : $123$,$132$,$312$,$213$,$231$,$321$. In this case, the minimal superpermutation is $123121321$. The minimal length is 9. Minimal Length An apparent lower bound of the length of superpermutation on $n$ symbols is $n ! +n-1$, because it must contain every permutation of the $n!$ permutations as a substring - the first permutation has $n$ characters to the string, and each of the remaining $n ! - 1$ permutations have a lengh of at least 1 character more. A trivial upper bound on the length of a minimal superpermutation is $ n* n !$ Suppose we already have a small superpermutation on $n$ and we want to construct a small superpermutation on $n+1$ symbols. To do so, simply replace each permutaiton in the $n$ symbol superpermutation by (1) the permutaiton, (2)the symbol $n+1$ (3) that permutaiton again. You can find detail here: link Uniqueness It turns out that there are in fact many superpermutations of the conjectured minimal lengh. The result of [1] shows that there are at least $\prod_{k=1}^{n-4} (n-k-2)^{k\cdot k!}$ Reference Non-uniqueness of minimal superpermutation</summary></entry></feed>